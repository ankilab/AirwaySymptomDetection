{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0af8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import flammkuchen as fl\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "from neural_networks.src.dataloader import DataLoader\n",
    "from src.conf_matrices import generate_confusion_matrix\n",
    "from src.roc_curves import generate_roc_curve\n",
    "from src.inference_time import get_inference_time\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.params import Params\n",
    "import copy\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "from neural_networks.src.ResNet import resnet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All paths\n",
    "PATH = \"../neural_networks/Results/\"\n",
    "# Audio\n",
    "PATH_RN18_AUDIO = PATH + \"MIC/ResNet18/20210830-133118/Checkpoints/model.hdf5\"\n",
    "PATH_RN34_AUDIO = PATH + \"MIC/ResNet34/20210830-133509/Checkpoints/model.hdf5\"\n",
    "PATH_EFNB0_AUDIO = PATH + \"MIC/EfficientNetB0/20210830-133837/Checkpoints/model.hdf5\"\n",
    "PATH_MNV2_AUDIO = PATH + \"MIC/MobileNetV2/20210830-134514/Checkpoints/model.hdf5\"\n",
    "PATH_RNNAMOH_AUDIO = PATH + \"MIC/RNN_Amoh/20210830-134906/Checkpoints/model.hdf5\"\n",
    "PATH_RNNBASIC_AUDIO = PATH + \"MIC/RNN_Basic/20210830-135050/Checkpoints/model.hdf5\"\n",
    "\n",
    "# NSA\n",
    "PATH_RN18_NSA = PATH + \"NSA/ResNet18/20210830-135221/Checkpoints/model.hdf5\"\n",
    "PATH_RN34_NSA = PATH + \"NSA/ResNet34/20210830-135440/Checkpoints/model.hdf5\"\n",
    "PATH_EFNB0_NSA = PATH + \"NSA/EfficientNetB0/20210830-135809/Checkpoints/model.hdf5\"\n",
    "PATH_MNV2_NSA = PATH + \"NSA/MobileNetV2/20210830-140436/Checkpoints/model.hdf5\"\n",
    "PATH_RNNAMOH_NSA = PATH + \"NSA/RNN_Amoh/20210830-140822/Checkpoints/model.hdf5\"\n",
    "PATH_RNNBASIC_NSA = PATH + \"NSA/RNN_Basic/20210830-140953/Checkpoints/model.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a586f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB_CLASSES = 4\n",
    "\n",
    "# load params from params.json\n",
    "params = Params(\"../neural_networks/params.json\")\n",
    "\n",
    "# load test data for evaluation\n",
    "params.n_mels_cnn = 64\n",
    "\n",
    "params.signal_type = \"MIC\"\n",
    "data_loader_mic = DataLoader(params=params, nb_classes=NB_CLASSES)\n",
    "X_test_mic, Y_test_mic = data_loader_mic.get_test_data()\n",
    "\n",
    "params.signal_type = \"NSA\"\n",
    "data_loader_nsa = DataLoader(params=params, nb_classes=NB_CLASSES)\n",
    "X_test_nsa, Y_test_nsa = data_loader_nsa.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test):\n",
    "    test_loss, test_acc = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for saving all results\n",
    "d = dict()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879547ed",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dae92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn18_audio = resnet.resnet_18(num_classes=4)\n",
    "rn18_audio.build(input_shape=(None, 64, 64, 1))\n",
    "rn18_audio.load_weights(PATH_RN18_AUDIO)\n",
    "rn18_audio.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "rn18_nsa = resnet.resnet_18(num_classes=4)\n",
    "rn18_nsa.build(input_shape=(None, 64, 64, 1))\n",
    "rn18_nsa.load_weights(PATH_RN18_NSA)\n",
    "rn18_nsa.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "d = dict(d, \n",
    "         rn18_test_acc=[evaluate_model(rn18_audio, X_test_mic[..., None], Y_test_mic), evaluate_model(rn18_nsa, X_test_nsa[..., None], Y_test_nsa)], \n",
    "         rn18_inf_time_cpu=[get_inference_time(rn18_audio, image_size=(1, 64, 64, 1), gpu=False), get_inference_time(rn18_nsa, image_size=(1, 64, 64, 1), gpu=False)],\n",
    "         rn18_inf_time_gpu=[get_inference_time(rn18_audio, image_size=(1, 64, 64, 1), gpu=True), get_inference_time(rn18_nsa, image_size=(1, 64, 64, 1), gpu=True)],\n",
    "         rn18_preds=[rn18_audio.predict(X_test_mic[..., None]), rn18_nsa.predict(X_test_nsa[..., None])],\n",
    "         rn18_nb_params=rn18_audio.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3196b8b",
   "metadata": {},
   "source": [
    "## ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ac278",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn34_audio = resnet.resnet_34(num_classes=4)\n",
    "rn34_audio.build(input_shape=(None, 64, 64, 1))\n",
    "rn34_audio.load_weights(PATH_RN34_AUDIO)\n",
    "rn34_audio.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "rn34_nsa = resnet.resnet_34(num_classes=4)\n",
    "rn34_nsa.build(input_shape=(None, 64, 64, 1))\n",
    "rn34_nsa.load_weights(PATH_RN34_NSA)\n",
    "rn34_nsa.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "d = dict(d, \n",
    "         rn34_test_acc=[evaluate_model(rn34_audio, X_test_mic[..., None], Y_test_mic), evaluate_model(rn34_nsa, X_test_nsa[..., None], Y_test_nsa)], \n",
    "         rn34_inf_time_cpu=[get_inference_time(rn34_audio, image_size=(1, 64, 64, 1), gpu=False), get_inference_time(rn34_nsa, image_size=(1, 64, 64, 1), gpu=False)],\n",
    "         rn34_inf_time_gpu=[get_inference_time(rn34_audio, image_size=(1, 64, 64, 1), gpu=True), get_inference_time(rn34_nsa, image_size=(1, 64, 64, 1), gpu=True)],\n",
    "         rn34_preds=[rn34_audio.predict(X_test_mic[..., None]), rn34_nsa.predict(X_test_nsa[..., None])],\n",
    "         rn34_nb_params=rn34_audio.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695326ad",
   "metadata": {},
   "source": [
    "## EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "efn_audio = efn.EfficientNetB0(input_shape=(64, 64, 1),\n",
    "                              include_top=True,\n",
    "                              weights=PATH_EFNB0_AUDIO,\n",
    "                              classes=NB_CLASSES)\n",
    "efn_audio.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "efn_nsa = efn.EfficientNetB0(input_shape=(64, 64, 1),\n",
    "                              include_top=True,\n",
    "                              weights=PATH_EFNB0_NSA,\n",
    "                              classes=NB_CLASSES)\n",
    "efn_nsa.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "d = dict(d, \n",
    "         efn_test_acc=[evaluate_model(efn_audio, X_test_mic[..., None], Y_test_mic), evaluate_model(efn_nsa, X_test_nsa[..., None], Y_test_nsa)], \n",
    "         efn_inf_time_cpu=[get_inference_time(efn_audio, image_size=(1, 64, 64, 1), gpu=False), get_inference_time(efn_nsa, image_size=(1, 64, 64, 1), gpu=False)],\n",
    "         efn_inf_time_gpu=[get_inference_time(efn_audio, image_size=(1, 64, 64, 1), gpu=True), get_inference_time(efn_nsa, image_size=(1, 64, 64, 1), gpu=True)],\n",
    "         efn_preds=[efn_audio.predict(X_test_mic[..., None]), efn_nsa.predict(X_test_nsa[..., None])], \n",
    "         efn_nb_params=efn_audio.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c170b",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab659dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnet_audio = tf.keras.applications.MobileNetV2(input_shape=(64, 64, 1),\n",
    "                                               include_top=True,\n",
    "                                               weights=PATH_MNV2_AUDIO,\n",
    "                                               classes=NB_CLASSES)\n",
    "mnet_audio.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "mnet_nsa = tf.keras.applications.MobileNetV2(input_shape=(64, 64, 1),\n",
    "                                             include_top=True,\n",
    "                                             weights=PATH_MNV2_NSA,\n",
    "                                             classes=NB_CLASSES)\n",
    "mnet_nsa.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "d = dict(d, \n",
    "         mnet_test_acc=[evaluate_model(mnet_audio, X_test_mic[..., None], Y_test_mic), evaluate_model(mnet_nsa, X_test_nsa[..., None], Y_test_nsa)], \n",
    "         mnet_inf_time_cpu=[get_inference_time(mnet_audio, image_size=(1, 64, 64, 1), gpu=False), get_inference_time(mnet_nsa, image_size=(1, 64, 64, 1), gpu=False)],\n",
    "         mnet_inf_time_gpu=[get_inference_time(mnet_audio, image_size=(1, 64, 64, 1), gpu=True), get_inference_time(mnet_nsa, image_size=(1, 64, 64, 1), gpu=True)],\n",
    "         mnet_preds=[mnet_audio.predict(X_test_mic[..., None]), mnet_nsa.predict(X_test_nsa[..., None])], \n",
    "         mnet_nb_params=mnet_audio.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9c144",
   "metadata": {},
   "source": [
    "## RNN Amoh (https://ieeexplore.ieee.org/abstract/document/7570164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_amoh_audio = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(128, input_shape=(64, 64), return_sequences=True),\n",
    "    tf.keras.layers.GRU(64, return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64)),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "rnn_amoh_nsa = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(128, input_shape=(64, 64), return_sequences=True),\n",
    "    tf.keras.layers.GRU(64, return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64)),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "rnn_amoh_audio.load_weights(PATH_RNNAMOH_AUDIO)\n",
    "rnn_amoh_nsa.load_weights(PATH_RNNAMOH_NSA)\n",
    "\n",
    "rnn_amoh_audio.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "rnn_amoh_nsa.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "d = dict(d, \n",
    "         rnn_amoh_test_acc=[evaluate_model(rnn_amoh_audio, X_test_mic, Y_test_mic), evaluate_model(rnn_amoh_nsa, X_test_nsa, Y_test_nsa)], \n",
    "         rnn_amoh_inf_time_cpu=[get_inference_time(rnn_amoh_audio, image_size=(1, 64, 64), gpu=False), get_inference_time(rnn_amoh_nsa, image_size=(1, 64, 64), gpu=False)],\n",
    "         rnn_amoh_inf_time_gpu=[get_inference_time(rnn_amoh_audio, image_size=(1, 64, 64), gpu=True), get_inference_time(rnn_amoh_nsa, image_size=(1, 64, 64), gpu=True)],\n",
    "         rnn_amoh_preds=[rnn_amoh_audio.predict(X_test_mic), rnn_amoh_nsa.predict(X_test_nsa)], \n",
    "         rnn_amoh_nb_params=rnn_amoh_audio.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27295c",
   "metadata": {},
   "source": [
    "## RNN Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25958821",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_basic_audio = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, input_shape=(64, 64), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
    "        ])\n",
    "\n",
    "rnn_basic_nsa = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, input_shape=(64, 64), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(NB_CLASSES, activation='softmax')\n",
    "        ])\n",
    "\n",
    "rnn_basic_audio.load_weights(PATH_RNNBASIC_AUDIO)\n",
    "rnn_basic_nsa.load_weights(PATH_RNNBASIC_NSA)\n",
    "\n",
    "rnn_basic_audio.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "rnn_basic_nsa.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "d = dict(d, \n",
    "         rnn_basic_test_acc=[evaluate_model(rnn_basic_audio, X_test_mic, Y_test_mic), evaluate_model(rnn_basic_nsa, X_test_nsa, Y_test_nsa)], \n",
    "         rnn_basic_inf_time_cpu=[get_inference_time(rnn_basic_audio, image_size=(1, 64, 64), gpu=False), get_inference_time(rnn_basic_nsa, image_size=(1, 64, 64), gpu=False)],\n",
    "         rnn_basic_inf_time_gpu=[get_inference_time(rnn_basic_audio, image_size=(1, 64, 64), gpu=True), get_inference_time(rnn_basic_nsa, image_size=(1, 64, 64), gpu=True)],\n",
    "         rnn_basic_preds=[rnn_basic_audio.predict(X_test_mic), rnn_basic_nsa.predict(X_test_nsa)],\n",
    "         rnn_basic_nb_params=rnn_basic_audio.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results using 'flammkuchen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl.save(\"results_evaluate_models.vfp\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5652c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
